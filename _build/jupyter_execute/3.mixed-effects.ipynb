{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f72679-d1d8-4dbe-90e6-df28aa84fd99",
   "metadata": {},
   "source": [
    "# Mixed-effects: The Best Way\n",
    "In order to improve upon the FFX approach, we need a method that accommodates the between-subjects variance. To see how to do this, let us return to our hierarchical model from earlier\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_{ij}  &= \\mu_{j} + \\epsilon_{ij} &\\quad \\text{(Level 1)} \\\\\n",
    "\\mu_{j} &= \\mu + \\eta_{j} &\\quad \\text{(Level 2)} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we collapse across the levels (by replacing $\\mu_{j}$ with its equality from Level 2), we create a single model\n",
    "\n",
    "$$\n",
    "y_{ij}  = \\mu + \\eta_{j} + \\epsilon_{ij}\n",
    "$$\n",
    "\n",
    "Importantly, this model contains a single parameter associated with the population mean (the $\\mu$ term) and *two* error terms ($\\eta_{j}$ and $\\epsilon_{ij}$). Because the population means is considered a *constant* (i.e. it does not change from measurement-to-measurement), it is known as a *fixed-effect*. As such, any random variation in the value of $y_{ij}$ must come from $\\eta_{j}$ and $\\epsilon_{ij}$. As such, these terms are both *random variables*, with distributions assumed to have the following form\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\eta_{j}       &\\sim \\mathcal{N}(0, \\sigma^{2}_{b})     \\\\\n",
    "\\epsilon_{ij}  &\\sim \\mathcal{N}(0, \\sigma^{2}_{w_{j}}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As such, these error terms capture both the *between-subject* and *within-subject* variances. Because they are random variables, they are termed *random-effects*. Because our model now contains both kinds of effects, it is known as a *mixed-effects* (MFX) model.\n",
    "\n",
    "```{admonition} Advanced: More About the MFX Model Terms\n",
    ":class: dropdown\n",
    "To understand the MFX model terms in more detail, recall the diagram of the between-subjects and within-subject variance in {numref}`variance-sources-fig`. The between-subject variance is quantified as the average vertical distance between the group mean and the subject mean, whereas the within-subject variance is quantified as the vertical distance between the subject mean and the raw data. These vertical distances are *exactly* what is being captured by the random-effects $\\eta_{j}$ and $\\epsilon_{ij}$. \n",
    "\n",
    "The $\\eta_{j}$ term is the difference between the group mean and the subject mean. As such, the mean for subject $j$ is given by\n",
    "\n",
    "$$\n",
    "\\bar{y}_{.j} = \\mu + \\eta_{j}.\n",
    "$$\n",
    "\n",
    "The $\\eta_{j}$ values are therefore exactly the same as the residuals from a regular GLM, except that they are defined at the level of each individual subject. In fact, if there were no repeated measurements then $\\eta_{j}$ would be *exactly the same* as the residuals from the GLM. Thus, in the MFX equation above, $\\mu$ is a constant of the group and $\\eta_{j}$ is a subject-specific deflections that indicated how far each subject is from the group mean. Because the subjects are a random sample, it therefore makes sense that the subject-specific deflections are also random and thus $\\eta_{j}$ is a *random-effect*.\n",
    "\n",
    "From this, we can infer that $\\epsilon_{ij}$ must be another error term, this time indicating the distance between the subject mean and the raw data. If we again think about $\\mu + \\eta_{j}$ giving the subject-specific mean, then the model can be thought-of as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_{ij} &= \\mu + \\eta_{j} + \\epsilon_{ij} \\\\\n",
    "       &= \\bar{y}_{.j} + \\epsilon_{ij}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Again, these are deflections that represent variations in the raw data that are not associated with the fact that we have different subjects, rather these represent sampling errors, mistakes or any other factor that would cause a subject to not respond *identically* every single time. Much like the $\\eta_{j}$, these are akin to the residuals of a regular GLM analysis. Because each repeat within a repeated-measurements design is effectively a random draw from a probability distribution, then it makes sense that these deflections are also random and this $\\epsilon_{ij}$ is also a *random-effect*.\n",
    "\n",
    "[Crowder & Hand (1990)](https://www.taylorfrancis.com/books/mono/10.1201/9781315137421/analysis-repeated-measures-martin-crowder-david-hand) described the terms in this model in a more poetic fashion that may be useful for understanding:\n",
    "- $\\mu$: \"An immutable constant of the universe\"\n",
    "- $\\eta_{j}$: \"A lasting characteristic of the individual\"\n",
    "- $\\epsilon_{ij}$ : \"A fleeting aberration of the moment\"\n",
    "\n",
    "Because $\\eta_{j}$ and $\\epsilon_{ij}$ are effectively two error terms, they can be used to calculate *variance*, in the same way that the errors of the GLM can be used to calculate variance. Remembering that the basic definition of variance is\n",
    "\n",
    "$$\n",
    "\\sigma^{2} = \\frac{\\sum{(y_{i} - \\bar{y})^{2}}}{n - 1},\n",
    "$$\n",
    "\n",
    "we only need to recognise that $y_{i}$ represents some data and $\\bar{y}$ is simply the mean of the distribution of that data. As such, when calculating $\\sigma^{2}_{b}$ we want the numerator to be the difference between the subject means and the group means, which we know is captured by $\\eta_{j}$. Thus the average sum-of-squared $\\eta_{j}$ terms gives us the estimate of the *between-subjects* variance. Similarly, if the numerator is the difference between the subject means and the raw data (i.e. $\\epsilon_{ij}$), then the average sum-of-squares gives us an estimate of the *within-subject* variance.\n",
    "```\n",
    "\n",
    "```{tip}\n",
    "In the world of fMRI, MFX analyses are often referred to as *random-effects* (RFX) models. This corresponds to the fact that we are treating subjects as a random draw from a population. The subjects are therefore seen as *random* rather than *fixed*. It is *not* implying that the model only contains random effects, though this would be the interpretation if this term was being used in the usual statistical sense. Just remember that RFX and MFX are used somewhat interchangeably in fMRI, just to make sure everyone is as confused as possible.\n",
    "```\n",
    "\n",
    "Importantly, collapsing the two levels together means that our overall probability model for an individual subject's data is given by\n",
    "\n",
    "$$\n",
    "y_{ij} \\sim \\mathcal{N}(\\mu, \\sigma^{2}_{b} + \\sigma^{2}_{w_{j}}).\n",
    "$$\n",
    "\n",
    "Thus, each data-point we sample can be thought of as containing some mixture of the *within-subject* and the *between-subject* variance. This collapsed sampling model is illustrated in {numref}`complete-sampling-model-fig`.\n",
    "\n",
    "```{figure} images/complete-sampling-model.png\n",
    "---\n",
    "width: 500px\n",
    "name: complete-sampling-model-fig\n",
    "---\n",
    "Illustration of the complete sampling model where each observation (indicated by a cross) is drawn from a population distribution (dashed line) that contains the individual subject distributions (solid lines). The variance of each observation around the group mean is therefore a combination of the variance *between* the subjects and the variance *within* a subject. This also demonstrated how the within-subject variance is much smaller than the between-subjects variance.\n",
    "```\n",
    "\n",
    "The advantage of MFX models is that they can *separate* these two sources. This has several practical advantages\n",
    "- The correct variance terms can be selected for testing different effects. For instance, effects associated with comparing groups need to use the *between-subject* variance, otherwise inference will be too liberal and we are more likely to detect false-positives.\n",
    "- Subjects who are *noisy* (i.e. who have larger values of $\\sigma^{2}_{w_{j}}$) can be *down-weighted* in the analysis. This means the model will automatically trust cleaner data sets and use the information from less-noisy subjects more than noisy subjects.\n",
    "\n",
    "As such, MFX approaches are highly beneficial because they can separate and quantify these difference sources of variance, allowing for a model that is much more accurate and much more flexible.\n",
    "\n",
    "## Computational Challenges for MFX\n",
    "Although a MFX analysis is clearly advantageous, there are unfortunately computational challenges when it comes to using these methods with fMRI data. Collapsing all our data together to form one large model has some major requirements in terms of computing resources. Fitting that amount of data in memory at once may be possible for a small number of subjects, but it soon becomes impractical for larger sample sizes. Furthermore, the simultaneous estimation of the variance source requires iterative maximum likelihood methods at each voxel. Not only can this be very slow, but there is the possibility of a solution not converging, leading to no results at some voxels. For instance, the investigations by [Guillaume *et al.* (2014)](https://pmc.ncbi.nlm.nih.gov/articles/PMC4073654/pdf/main.pdf) showed anywhere from 7 hours to over 9 days for a MFX analysis to complete, with many voxels failing to converge.\n",
    "\n",
    "One solution to this problem is lean on the hierarchical approach described earlier. Using this method, we estimate the GLM of each subject separately and then take contrasts of the parameter estimates from each subject through to a 2nd-level GLM analysis. This effectively breaks the computation into smaller chunks, with much lower memory requirements. The contrasts from the 1st-level analysis forms the raw data at the 2nd-level. This is shown in {numref}`multilevel-fig` from [Poldrack, Mumford and Nichols (2011)](https://www.librarysearch.manchester.ac.uk/discovery/fulldisplay?context=L&vid=44MAN_INST:MU_NUI&search_scope=MyInst_and_CI&tab=Everything&docid=alma992975905221601631). \n",
    "\n",
    "```{figure} images/multilevel-poldrack.png\n",
    "---\n",
    "width: 800px\n",
    "name: multilevel-fig\n",
    "---\n",
    "Illustration of how the multi-level framework can be implemented in two stages when working with fMRI data.\n",
    "```\n",
    "\n",
    "In this example, subjects from two different groups took part in a task looking at pictures of `faces` and pictures of `houses`. The effect of interest for each subject is the difference in activation between `faces` and `houses`. This difference is calculated for each subject and then taken through to a group-level model, where the average difference for the first group is compared to the average difference for the second group. We will explain how this 2nd-level model is structured later. For now, just try and get a sense of the general concept here.\n",
    "\n",
    "### The Problem of Single-subject Variances\n",
    "The challenge for performing the analysis this way is taking the variance estimates for each subject through to the group-level. Although simple enough to estimate each subject model using least-squares, the power of a MFX analysis comes from its ability to use those single-subject estimates to inform the estimation of the group-level effects. As it turns out, the integration of the single-subject variances at the group-level is far from trivial. Solutions hasve been given by [Beckmann, Jenkinson & Smith (2003)](https://pubmed.ncbi.nlm.nih.gov/14568475/), who used a complicated Bayesian approach to combine the variance estimates. This is implemented as the `FLAME` algorithm in `FSL`. Another solution was given more recently by [Chen *et al.* (2012)](https://pubmed.ncbi.nlm.nih.gov/22245637/), adapting a method from the world of meta-analyses to achieve similar results. This is implemented as `3dMEMA` in `AFNI`. Conspicuously, `SPM` is missing from this list. Now, it is unfair to say the `SPM` cannot do MFX, because there is a tool buried inside `SPM` for this (as described by [Friston *et al.*, 2005](https://pubmed.ncbi.nlm.nih.gov/22245637/)). However, this has never been presented as a major part of the `SPM` software. This is because the way that the `SPM` authors actually want you to do a group-level analyses is using a method known as the *summary statistics approach*, which will be the focus of the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e10e7a-5342-4f40-8e60-f0e4f7b13a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MATLAB Kernel",
   "language": "matlab",
   "name": "jupyter_matlab_kernel"
  },
  "language_info": {
   "file_extension": ".m",
   "mimetype": "text/x-matlab",
   "name": "matlab"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}